理论上，网络数据采集是一种通过多种手段收集网络数据的方式，不光是通过与API 交互（或者直接与浏览器交互）的方式。最常用的方法是写一个自动化程序向网络服务器请求数据（通常是用HTML 表单或其他网页文件），然后对数据进行解析，提取需要的信息。

实践中，网络数据采集涉及非常广泛的编程技术和手段，比如数据分析、信息安全等。本书将在第一部分介绍关于网络数据采集和网络爬行（crawling）的基础知识，一些高级主题放在第二部分介绍。

# 第一部分 创建爬虫

这部分内容重点介绍网络数据采集的基本原理：如何用Python 从网络服务器请求信息，如何对服务器的响应进行基本处理，以及如何以自动化手段与网站进行交互。最终，你将轻松游弋于网络空间，创建出具有域名切换、信息收集以及信息存储功能的爬虫。

这部分内容涵盖了一般人（也包括技术达人）在思考“网络爬虫”时通常的想法：

- 通过网站域名获取HTML 数据
- 根据目标信息解析数据
-  存储目标信息
- 如果有必要，移动到另一个网页重复这个过程

## 第一章 初见网络爬虫

### 1.1 网络连接

urllib：Python的标准库，包含了从网络请求数据，处理cookie，甚至改变像请求头和用户代理这些元数据的函数。

urlopen：用来打开并读取一个从网络获取的远程对象

### 1.2 BeautifulSoup

它通过点位HTML标签来格式化和组织复杂的网络信息，用简单易用的Python对象为我们XML结构信息。

## 第二章 复杂HTML解析

解析复杂的HTML网页时需要使用技巧将网页上那些不需要的信息敲掉。

### 2.1 不是一直都要用锤子

在解析复杂的HTML页面时需要避免的问题。

假如你已经确定了目标内容，可能是采集一个名字、一组统计数据，或者一段文字。你的目标内容可能隐藏在一个HTML“烂泥堆”的第20 层标签里，带有许多没用的标签或HTML 属性。假如你不经考虑地直接写出下面这样一行代码来抽取内容：

```Python
bsObj.findAll("table")[4].findAll("tr")[2].find("td").findAll("div")[1].find("a")
```

虽然也可以达到目标，但这样看起来并不是很好。除了代码欠缺美感之外，还有一个问题是，当网站管理员对网站稍作修改之后，这行代码就会失效，甚至可能会毁掉整个网络爬虫。那么你应该怎么做呢？

- 寻找“打印此页”的链接，或者看看网站有没有HTML样式更友好的移动版
- 寻找隐藏在JavaScript文件里的信息。要实现这一点，你可能需要查看网页加载的JavaScript文件。我曾经要把一个网站上的街道地址（以经度和纬度呈现的）整理成格式整洁的数组时，查看过内嵌谷歌地图的JavaScript文件，里面有每个地址的标记点。
- 虽然网页标题经常会用到，但是这个信息也许可以从网页的URL链接里获取。
- 如果你要找的信息只存在于一个网站上，别处没有，那你确实是运气不佳。如果不只限于这个网站，那么你可以找找其他数据源。有没有其他网站也显示了同样的数据？网站上显示的数据是不是从其他网站上抓取后攒出来的？

**尤其是在面对埋藏很深或格式不友好的数据时，千万不要不经思考就写代码，一定要三思而后行**。如果你确定自己不能另辟蹊径，那么本章后面的内容就是为你准备的。

### 2.2 再端一碗BeautifulSoup

在这一节，我们将介绍通过属性查找标签的方法，标签组的使用，以及标签解析树的导航过程。

网络爬虫可以通过class属性的值，轻松地区分出两种不同的标签。例如，它们可以用BeautifulSoup抓取网页上所有的红色文字，而绿色文字一个都不抓。

#### 2.2.1 BeautifulSoup的find和findAll()

这两个函数非常相似，BeautifulSoup文档里两者的定义就是这样：

```Python
findAll(tag, attributes, recursive, text, limit, keywords)
find(tag, attributes, recursive, text, keywords)
```

标签参数tag前面已经介绍过——你可以传一个标签的名称或多个标签名称组成的Python列表做标签参数。例如，下面的代码将返回一个包含HTML文档中所有标题标签的列表：

```Python
.findAll({"h1","h2","h3","h4","h5","h6"})
```

属性参数attributes是用一个Python字典封装一个标签的若干属性和对应的属性值。例如，下面这个函数会返回HTML文档里红色与绿色两种颜色的span标签：

```Python
.findAll("span", {"class":{"green", "red"}})
```

递归参数recursive是一个布尔变量。你想抓取HTML文档标签结构里多少层的信息？如果recursive设置为True，findAll就会根据你的要求去查找标签参数的所有子标签，以及子标签的子标签。如果recursive设置为False，findAll就只查找文档的一级标签。findAll默认是支持递归查找的（recursive默认值是True）；一般情况下这个参数不需要设置，除非你真正了解自己需要哪些信息，而且抓取速度非常重要，那时你可以设置递归参数。

文本参数text有点不同，它是用标签的文本内容去匹配，而不是用标签的属性。假如我们想查找前面网页中包含“theprince”内容的标签数量，我们可以把之前的findAll方法换成下面的代码：

```Python
nameList = bsObj.findAll(text="the prince")
print(len(nameList))
// 结果为7
```

范围限制参数limit，显然只用于findAll方法。find其实等价于findAll的limit等于1时的情形。如果你只对网页中获取的前x项结果感兴趣，就可以设置它。但是要注意，这个参数设置之后，获得的前几项结果是按照网页上的顺序排序的，未必是你想要的那前几项。

还有一个关键词参数keyword，可以让你选择那些具有指定属性的标签。例如：

```Python
allText = bsObj.findAll(id="text")
print(allText[0].get_text())
```

#### 2.2.2 其他BeautifulSoup对象

- BeautifulSoup对象
- 标签Tag对象
- NavigableString对象：用来表示标签里的文字，不是标签
- Comment对象：用来查找HTML文档的注释标签，<!-- like this -->

#### 2.2.3 导航树

**1.处理字标签和其代标签**

一般情况下，BeautifulSoup函数总是处理当前标签的后代标签。例如，bsObj.body.h1选择了body标签后代里的第一个h1标签，不会去找body外面的标签。

类似地，bsObj.div.findAll("img")会找出文档中第一个div标签，然后获取这个div后代里所有的img标签列表。

如果你只想找出子标签，可以用.children标签。

**2.处理兄弟标签**

next_siblings() 函数，获取当前标签的之后的兄弟标签，不包括自己。

previous_siblings函数，获取当前标签的之前的兄弟标签，不包括自己。

还有next_sibling和previous_sibling函数，与next_siblings和previous_siblings的作用类似，只是它们返回的是单个标签，而不是一组标签。

**3.父标签处理**

parent和parents。

### 2.3 正则表达式和BeautifulSoup

```python
images = bsObj.findAll("img",{"src":re.compile("\.\.\/img\/gifts/img.*\.jpg")})
```

通过re来撇皮正则表达式。

### 2.4 获取属性

对于一个标签对象，可以用下面的代码获取它的全部属性：

myTag.attrs

要注意这行代码返回的是一个Python字典对象，可以获取和操作这些属性。比如要获取图片的资源位置src，可以用下面这行代码：

myImgTag.attrs["src"]

### 2.5 Lambda表达式

BeautifulSoup允许我们把特定函数类型当作findAll函数的参数。唯一的限制条件是这些函数必须把一个标签作为参数且返回结果是布尔类型。BeautifulSoup用这个函数来评估它遇到的每个标签对象，最后把评估结果为“真”的标签保留，把其他标签剔除。

例如，下面的代码就是获取有两个属性的标签：

```Python
soup.findAll(lambda tag: len(tag.attrs) == 2)
```

### 2.6 超越BeautifulSoup

- lxml：可以用来解析HTML和XML文档，以非常底层的实现而闻名于世。
- HTML parser：Python自带的解析库。

## 第三章 开始采集

### 3.1 用Scrapy采集

Scrapy就是一个帮你大幅降低网页链接查找和识别工作复杂度的Python库，它可以让你轻松地采集一个或多个域名的信息。

Scrapy用Item对象决定要从它浏览的页面中提取哪些信息。Scrapy支持用不同的输出格式来保存这些信息，比如CSV、JSON或XML。

当然你也可以自定义Item对象，把结果写入你需要的一个文件或数据库中，只要在爬虫的parse部分增加相应的代码即可。

Scrapy是处理网路数据采集相关问题的利器。它可以自动收集所有URL，然后和指定的规则进行比较；确保所有的URL是唯一的；根据需求对相关的URL进行标准化；以及到更深层的页面中递归查找。

## 第四章 使用API

### 4.1 API通用规则

#### 4.1.1 方法

利用HTTP从网络服务获取信息有四种方式：

- GET：是在输入API之后从网站获取数据
- POST：基本就是当你填写表单或提交信息到网络服务器的后端程序时所做的事情
- PUT：用来更新一个对象或信息
- DELETE：用于删除一个对象

