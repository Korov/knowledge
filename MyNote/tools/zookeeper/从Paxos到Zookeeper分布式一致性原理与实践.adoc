= 从Paxos到Zookeeper分布式一致性原理与实践 =
Korov9 <korov9@163.com>
v1.0 2021-08-04
:toc: right
:table-caption!:

== 分布式架构 ==

=== 分布式的特点 ===

分布式系统的定义：分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统。

一个标准的分布式系统在没有任何特定业务逻辑约束的情况下，都会有如下几个特征

- **分布性**：分布式系统中的多台计算机都会在空间上随意分布，同时，机器的分布情况也会随时变动
- **对等性**：分布式系统中的计算机没有主从之分，组成分布式系统的所有计算机节点都是对等的。副本（Replica）是分布式系统对数据和服务提供的一种冗余方式。副本分为数据副本和服务副本。数据副本是指在不同的节点上持久化同一份数据，当某一节点上存储的数据丢失时，可以从副本上读取到该数据，这是解决分布式系统数据丢失问题最有效的手段。服务副本，指多个节点提供同样的服务，每个节点都有能力接收来自外部的请求并进行相应的处理。
- **并发性**：如何准确并高效地协调分布式并发操作也成为了分布式系统架构与设计中最大的挑战之一。
- **缺乏全局时钟**：一个典型的分布式系统是由一系列在空间上随意分布的多个进程组成的，具有明显的分布性，这些进程之间通过交换信息来进行相互通信。因此在分布式系统中，很难定义两个事件究竟谁先谁后，原因就是因为分布式系统缺乏一个全局的始终序列控制。
- **故障总是会发生**：组成分布式系统的所有计算机，都有可能发生任何形式的故障。一个被大量工程实践所检验过的黄金定理是：任何在设计阶段考虑到的异常情况，一定会在系统实际运行中发生，并且，在系统实际运行过程中还会遇到很多在设计时未能考虑到的异常故障。所以除非需求指标允许，在系统设计时不能放过任何异常情况。

=== 分布式环境的各种问题 ===

==== 通信异常 ====

从集中式向分布式演变的过程中，必然引入了网络因素，网络是不可靠的。分布式系统需要在各个节点之间进行网络通信，因此每次网络通信都会伴随着网络不可用的风险。另外，即使分布式系统各节点之间的网络通信能够正常进行，其延时也会远大于单机操作。在现代计算机体系结构中，单机内存访问的延时在纳秒数量级（通常是10ns左右），而正常的一次网络通信延迟在0.1-1ms左右（相差100倍），会影响消息的收发过程，因此 **消息丢失** 和 **消息延迟** 变得非常普遍。

==== 网络分区 ====

当网络发生异常，导致分布式系统中部分节点之间的网络延时不断增大，最终导致组成分布式系统中的所有节点中，只有部分节点之间能够进行正常通信，而另一些节点则不能--这种现象称为 **网络分区** ，俗称脑裂。当网络分区出现时，分布式系统会出现局部小集群，在极端情况下，这些局部小集群会独立完成原本需要整个分布式系统才能完成的功能，包括对数据的事务处理，这就对分布式一致性提出了非常大的挑战。

==== 三态 ====

在分布式环境下，网络可能会出现各种各样的问题，因此分布式系统的每一次请求与响应，存在特有的“三态”概念，即成功、失败与超时。超时有两种情况：

- 由于网络原因，该请求并没有被成功的发送到接收方，而是在发送过程就发生了消息丢失现象；
- 该请求成功的被接收方接收后，并进行了处理，但是在将响应反馈给发送方的过程中，发生了消息丢失现象。

当发生这样的超时现象时，网络通信的发起方是无法确定当前请求是否被成功处理的。

==== 节点故障 ====

指分布式系统的服务器节点出现宕机或者僵死现象。通常根据经验来说，每个节点都有可能出现故障，并且每天都在发生。

=== CAP和BASE理论 ===

==== CAP定理 ====

CAP理论告诉我们，一个分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容错性（P：Partition tolerance）这三个基本需求，最多只能同时满足其中的两项。

. 一致性：在分布式环境中，一致性是指数据在多个副本之间是否能够保持一致的特性。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。在分布式系统中，如果能做到针对一个数据项的更新操作执行成功后，所有的用户都可以读到最新的值，那么这样的系统就被认为具有强一致性
. 可用性：系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果（成功或失败的结果，而不是让用户困惑的结果）。
. 分区容错性：分布式系统在遇到任何网络分区故障的时候，任然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。

==== BASE理论 ====

Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的简写。BASE是对CAP中一致性和可用性权衡的结果，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。

. 基本可用：分布式系统在出现不可预知故障的时候，允许损失部分可用性
.. 响应时间上的损失：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户查询结果，但由于出现故障，查询结果的响应时间增加到了1-2秒
.. 功能上的损失：正常情况下，在一个电子上午网站上进行购物，消费者几乎能够顺利地完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面
. 弱状态：弱状态也称为软状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时
. 最终一致性：最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。

在实际工程实践中，最终一致性存在一下五类主要变种：

. 因果一致性（Causal consistency）：如果进程A在更新某个数据项后通知了进程B，那么进程B之后对该数据项的访问都应该能够获取到进程A更新后的最新值，并且如果进程B要对该数据项进行更新操作的话，务必基于进程A更新后的最新值，即不能发生丢失更新的情况。
. 读己之所写（Read your writes）：进程A更新一个数据项之后，它自己总是能够访问到更新过的最新值，而不会看到旧值。
. 会话一致性（Session consistency）：会话一致性将对系统数据的访问过程框定在一个会话中：系统能保证在同一个有效的会话中实现“读己之所写”的一致性
. 单调读一致性（Monotonic read consistency）：如果一个进程从系统中读取一个数据项的某个值后，那么系统对于该进程后续的任何数据访问都不应该返回更旧的值。
. 单调写一致性（Monotonic write consistency）：一个系统需要能够确保来自同一个进程的写操作被顺序的执行。

== 一致性协议 ==

=== 2PC与3PC ===

当一个事务操作需要跨越多个分布式节点的时候，为了保持事务处理的ACID特性，就需要引入一个称为 **协调者（Coordinator）** 的组件来统一调度所有分布式节点的执行逻辑，这些被调度的分布式节点则被称为 **参与者（Participant）** 。协调者负责调度参与者的行为，并最终决定这些参与者是否要把事务真正的进行提交。

==== 2PC ====

Two-Phase Commit的缩写，即二阶段提交，是计算机网络尤其是在数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务处理过程中能够保持原子性和一致性而设计的一种算法。目前绝大部分的关系型数据库都是采用二阶段提交协议来完成分布式事务的处理。

二阶段提交协议是将事务的提交过程分成了两个阶段来进行处理，其执行流程如下：

. 阶段一：提交事务请求

.. 事务询问：协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应

.. 执行事务：各参与者节点执行事务操作，并将Undo和Redo信息计入事务日志中

.. 个参与者向协调者反馈事务询问的响应：如果参与者成功执行了事务操作，那么就会反馈给协调者Yes响应，表示事务可以执行；如果参与者没有成功执行事务，那么就反馈给协调者No响应，表示事务不可以执行

. 阶段二：执行事务提交。协调者会根据各参与者的反馈情况来决定最终是否可以进行事务提交操作，正常情况下，包含以下两种可能

.. 执行事务提交：假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务提交：
... 发送提交请求：协调者向所有参与者节点发出Commit请求

... 事务提交：参与者接收到Commit请求后，会正式执行事务提交操作，并在完成提交之后释放整个事务执行期间占用的事务资源

... 反馈事务提交结果：参与者在完成事务提交之后，向协调者发送Ack消息

... 完成事务：协调者接收到所有参与者反馈的Ack消息后，完成事务

.. 中断事务：假如任何一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务：

... 发送回滚请求：协调者向所有参与者节点发出Rollback请求

... 事务回滚：参与者接收到Rollback请求后，会利用其在阶段一中记录的Undo信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源

... 反馈事务回滚的结果：参与者在完成事务回滚之后，向协调者发送Ack消息

... 中断事务：协调者接收到所有参与者反馈的Ack消息后，完成事务中断

二阶段提交将一个事务的处理过程分为了投票和执行两个阶段，其核心是对每个事务都采用先尝试提交的处理方式，因此可以将二阶段提交看作一个强一致性的算法

优缺点：

. 优点：原理简单，实现方便。
. 缺点：同步阻塞、单点问题、脑裂、太过保守。
.. 同步阻塞：二阶段提交协议存在的最明显也是最大的一个问题就是同步阻塞，这会极大的限制分布式系统的性能。在二阶段提交的执行过程中，所有参与该事务的逻辑都处于阻塞状态，也就是说，各个参与者在等待其他参与者响应的过程中，将无法进行其他任何操作
.. 单点问题：协调者在二阶段提交协议中起到了非常重要的作用。一旦协调者出现问题，那么整个二阶段提交流程将无法运行
.. 数据不一致：在二阶段提交协议的阶段二，即执行事务提交的时候，当协调者向所有的参与者发送Commit请求之后，发生了局部网络异常或者是协调者在尚未发送完Commit请求之前自身发生了崩溃，导致最终只有部分参与者收到了Commit请求。于是，这部分收到了Commit请求的参与者就会进行事务的提交，而其他没有收到Commit请求的参与者则无法进行事务提交，于是整个分布式系统便出现了数据不一致性现象
.. 太过保守：如果协调者指示参与者进行事务提交询问的过程中，参与者出现故障而导致协调者始终无法获取到所有参与者的响应信息的话，这是协调者只能依靠自身的超时机制来判断是否需要中断事务，这样的策略显得比较保守。二阶段提交协议没有较为完善的容错机制，任意一个节点的失败都会导致整个事务的失败。

==== 3PC ====

Three-Phase Commit，三阶段提交将二阶段提交协议的“提交事务请求”过程一分为二，形成了由CanCommit、PreCommit和do Commit三个阶段组成的事务处理协议。

. 阶段一：CanCommit
.. 事务询问：协调者向所有的参与者发送一个包含事务内容的canCommit请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应
.. 各参与者向协调者反馈事务询问的响应：参与者在接收到来自协调者的canCommit请求后，正常情况下，如果其自身认为可以顺利执行事务，那么会反馈Yes响应，并进入预备状态，否则反馈No响应
. 阶段二：PreCommit
.. 执行事务预提交：假如协调者从所有的参与者获得的反馈都是Yes响应，就会执行预提交
... 发送预提交请求：协调者向所有参与者节点发出preCommit请求，并进入Prepared阶段
... 事务预提交：参与者接收到preCommit请求后，会执行事务操作，并将Undo和Redo信息记录到事务日志中
... 各参与者向协调者反馈事务执行的响应：如果参与者成功执行了事务操作，那么就会反馈给协调者Ack响应，同时等待最终的指令：提交（commit）或终止（abort）
.. 中断事务：如果任何一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务
... 发送中断请求：协调者向所有参与者节点发出abort请求
... 中断事务：无论是收到来自协调者的abort请求，或者是在等待协调者请求过程中出现超时，参与者都会中断事务
. 阶段三：doCommit。该阶段将进行真正的事务提交
.. 执行提交
... 发送提交请求：进入这一阶段，假设协调者处于正常工作状态，并且它接收到了来自所有参与者的Ack响应，那么它将从“预提交”状态转换到“提交”状态，并向所有参与者发送doCommit请求
... 事务提交：参与者接收到doCommit请求后，会正式执行事务提交操作，并在完成提交之后释放整个事务执行期间占用的事务资源
... 反馈事务提交结果：参与者在完成事务提交之后，向协调者发送Ack消息
... 完成事务：协调者接收到所有参与者反馈的Ack消息后，完成事务
.. 中断事务：假设协调者处于正常工作状态，并且有任意一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务
... 发送中断请求：协调者向所有的参与者节点发送abort请求
... 事务回滚：参与者接收到abort请求后，会利用其在阶段二中记录的Undo信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源
... 反馈事务回滚结果：参与者在完成事务回滚之后，向协调者发送Ack消息
... 中断事务：协调者接收到所有参与者反馈的Ack消息后，中断事务

****
需要注意的是，一旦进入阶段三，可能会存在以下两种故障

- 协调者出现问题
- 协调者和参与者之间的网络出现故障

无论出现那种情况，最终都会导致参与者无法及时接收到来自协调者的doCommit或是abort请求，针对这样的异常情况，参与者都会在等待超时之后继续进行事务提交
****

优缺点：

. 优点：相较于二阶段提交协议，三阶段提交协议最大的优点是降低了参与者的阻塞范围，并且能够在出现单点故障后继续达成一致
. 缺点：三阶段提交协议在去除阻塞的同时也引入了新的问题，那就是在参与者接收到preCommit消息后，如果网络出现分区，此时协调者所在的节点和参与者无法进行正常的网络通信，在这种情况下，该参与者依然会进行事务的提交，这必然会出现数据的不一致

=== Paxos算法详解 ===

==== 问题描述 ====

假设有一组可以提出提案的进程合集，那么对于一个一致性算法来说需要保证以下几点：

- 在这些被提出的提案中，只有一个会被选定
- 如果没有提案被提出，那么就不会有被选定的提案
- 当一个提案被选定后，进程应该可以获取被选定的提案信息

对于一致性来说，安全性需求如下：

- 只有被提出的提案才能被选定
- 只能有一个值被选定
- 如果某个进程认为某个提案被选定了，那么这个提案必须是真的被选定的那个

从整体上来说，Paxos算法的目标就是要保证最终有一个提案会被选定，当提案被选定后，进程最终也能获取到被选定的提案。

在该一致性算法中，有三种参与角色，我们用Proposer、Acceptor和Learner来表示。在具体的实现中，一个进程可能充当不止一种角色，在这里我们并不关心进程如何映射到各种角色，假设不同参与者之间可以通过收发消息来进行通信那么：

- 每个参与者以任意速度执行，可能会因为出错而停止，也可能会重启。同时，即使一个提案被选定后，所有的参与者也都可能失败或重启，因此除非那些失败或重启的参与者可以记录某些信息，否则将无法确定最终的值
- 消息在传递的过程中可能会出现不可预知的延迟，也可能会重复或丢失，但是消息不会被损坏，即消息内容不会被篡改。

== Zookeeper的典型应用场景

=== 数据发布订阅

数据发布/订阅系统（Publish/Subscribe）系统，即所谓的配置中心，就是发布者将数据发布到Zookeeper的一个或一系列节点上，供订阅者进行数据订阅，进而达到动态获取数据的目的，实现配置信息的集中式管理和数据的动态更新

Zookeeper采用推拉结合的方式：客户端向服务端注册自己需要关注的节点，一旦该节点的数据发生变更，那么服务器端就会向相应的客户端发送Wtacher事件通知，客户端接收到这个消息通知之后，需要主动到服务端获取最新的数据。 

NOTE: 这种方式好麻烦啊，没有通知到就不会去主动拉去消息，获取消息相对来说更及时，但是一个获取一个消息需要两个网络请求才能完成，浪费

****
- 推模式：服务端将数据主动发送给客户端
- 拉模式：客户端主动轮询服务端拉取数据

推模式下，服务端不知道客户端的消费速度，当服务端的发送速度大于客户端的消费速度时会导致客户端崩溃，但是消息发送会很及时。拉取模式下，客户端必须不断轮询服务端是否有可消费的消息，而且消费消息没有那么及时，但是可以更具自身能力决定拉取消息的速率。
****

=== 命名服务

Zookeeper实现全局唯一ID的生成。调用Zookeeper节点创建的API接口可以创建一个顺序节点，并且在API返回值中会返回这个节点的完整名字。利用这个特性，我们就可以借助Zookeeper来生成全局唯一ID了。

步骤：

. 所有客户端都会根据自己的任务类型，在指定类型的任务下面通过调用 `create()` 接口来创建一个顺序节点，例如创建 `job-` 节点
. 节点创建完毕后， `create()` 接口会返回一个完整的节点名，例如 `job-0000000003`
. 客户端拿到这个返回值后，拼接上 `type` 类型，例如 `type-job-0000000003` 这样就可以作为一个全局唯一的ID了。

=== Master选举

利用Zookeeper的强一致性，能够很好的保证在分布式高并发情况下节点的创建一定能够保证全局唯一性，利用这个特性就能很容易在分布式环境中进行Master选举了。

客户端集群会在zookeeper上创建一个临时节点，例如 `/master/service`，在这个过程中只有一个客户端能够成功创建这个节点，那么这个客户端所在的机器就成为了Master。同时其他没有在zookeeper上成功创建节点的客户端都会在节点 `/master/service` 上注册一个子节点变更的 `Watcher` ，用于监控当前的Master机器是否存活，一旦发现当前的Master挂了，那么其余的客户端将会重新进行Master选举。

NOTE: LeaderLatch:根据一个路径，多台客户端在该路径下创建临时顺序节点，例如： `/leader/node_1`,`/leader/node_2`,`/leader/node_3` 节点编号最小的客户端成为leader，没抢到leader的节点都监听前一个节点的删除事件，在前一个节点删除后重新抢主。这样就会按照临时节点的顺序一次成为master。

=== 分布式锁

==== 排他锁

排他锁（Exclusive Locks），又成为写锁或独占锁，是一种基本的锁类型。排他锁的核心是如何保证当前有且仅有一个事务获得锁，并且锁倍释放后，所有正在等待获取锁的事务都能够被通知到。

实现：所有客户端都会试图创建一个临时节点 `/exclusive_lock/lock` ，如果创建成功就可以任务该客户端获取了锁。同时所有没有获取到锁的客户端就需要到 `/exclusive_lock` 节点上注册一个子节点变更的Watcher监听，以便实时监听lock节点的变更情况。

因为 `/exclusive_lock/lock` 是一个临时节点，因此以下两种情况，都有可能释放锁。

- 当前获取锁的客户端机器发生宕机，那么zookeeper上的这个临时节点就会倍移除
- 正常执行完业务逻辑后，客户端就会主动将自己创建的临时节点删除。

==== 共享锁

共享锁（Shared Locks）又称为读锁，如果事务T1对数据对象O1加上了共享锁，那么当前事务只能对O1进行读取操作，其他事务也只能对这个数据对象加共享锁，直到该数据对象上的所有共享锁都被释放。

这里使用临时顺序节点来表示一个共享锁。因为每次创建一个临时顺序节点，就会给这个节点递增的序号。在需要获取共享锁时，所有客户端都会到 `/shared_lock` 这个节点下面创建一个临时顺序节点，如果当前是读请求，那么就创建例如 `/shared_lock/192.168.0.1-R-00000001` 的节点；如果是写请求，那么就创建例如 `/shared_lock/192.168.0.1-W-00000001` 的节点（ **R和W将会共享递增的序列号** ）。

通过zookeeper的节点来确定分布式读写顺序大致可以分为一下4个步骤：

. 创建完节点后，获取 `/shared_lock` 节点下所有子节点，并对该节点注册子节点变更的Watcher监听。
. 确定自己的节点序号所在子节点中的顺序
. 对于读请求：如果没有比机子序号小的子节点，或是所有比自己序号小的子节点都是读请求，那么表明自己已经成功获取了共享锁，同时开始执行读取逻辑。如果比自己序号小的子节点中有写请求，那么就需要进入等待；对于写请求：如果自己不是序号最小的子节点，那么就进入等待。
. 接受到Watcher通知后，重复步骤1.

但是步骤3中Watcher会给所有客户端发送通知，然后所有客户端都会去获取所有子节点，然后判断顺序，之后一个会进行下一步操作，其他的则是继续等到，在超大集群中会造成zookeeper压力过大。改进如下：

 . 客户端创建临时顺序节点
 . 客户端获取所有已经创建的子节点列表，注意这里不注册任何Watcher。
 . 注册Watcher，读请求：向比自己序号小的最后一个写请求节点注册Watcher监听；写请求：向比自己序号小的最后一个节点注册Watcher监听。
 
 . 等待Watcher通知，继续步骤2

== 自我总结

亲自测试了一下，相同的机器，相同的环境， `zookeeper` 部署在同一台机器，获取锁和释放锁一整个流程，zookeeper的分布式锁获取和释放一个锁需要的时间是本地可重入锁的70倍。而redis的可冲入锁花费的时间是本地锁的11倍

redis哨兵集群模式一主两从，三哨兵花费时间是普通锁的11倍

zookeeper三个集群锁的时间是普通锁的127倍

[source, text]
----
zk cluster 1634567533559:Clinet 1 has the lock, count:57046, cost:99353   1.74162956211   127
local 1634567582284:Clinet 4 has the lock, count:2013081, cost:27588   0.0137043665903
zk standalone 1634568057042:Clinet 2 has the lock, count:122415, cost:118490    0.96763635833  70
redis cluster 1634568648123:Clinet 0 has the lock, count:640216, cost:101315   0.15825127769377834   11
reids standalone  1634568904952:Clinet 1 has the lock, count:383682, cost:60710  0.15822999254591041 11

local 1943 2039  1991
zookeeper 113163  114140  113651.5          57
zookeeper cluster 204977  185368  195172.5  98
redis 17483 17643   17563                   8
----