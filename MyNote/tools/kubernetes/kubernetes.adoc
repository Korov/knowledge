= Kubernetes权威指南 =
Korov9 <korov9@163.com>
v1.0 2021-02-24
:imagesdir: picture
:toc: right

== Kubernetes的基本概念和术语 ==

Kubernetes中的大部分概念如Node,Pod,Replication Controller,Service等都可以被看作一种资源对象，几乎所有资源对象都可以通过Kubernetes提供的kubectl工具执行增删改查等操作并将其保存在etcd中持久化存储。从这个角度来看，Kubernetes其实是一个高度自动化的资源控制系统，他通过跟踪对比etcd库里保存的“资源期望状态”与当前环境中的“实际资源状态”的差异来实现自动控制和自动纠错的高级功能。

Kubernetes平台采用了**核心+外围扩展**的设计思路，在保持平台核心稳定的同时具备持续演进升级的优势。Kubernetes大部分常见的核心资源对象都归属于 `v1` 这个核心 `API` ，比如 `Node,Pod,Service,Endpoints,Namespace,RC,PersistenVolumn` 等。在 `1.9` 版本之后引入了 `apps/v1` 这个正式的扩展 `API` 组。

=== Master ===

Kubernetes里的Master指的是集群控制节点，在每个Kubernetes集群里都需要有一个Master来负责整个集群的管理和控制，基本上Kubernetes的所有控制命令都发给它，它负责具体的执行过程。Master通常会占据一个独立的服务器（高可用部署建议用3台服务器）。

在Master上运行这以下关键进程：

. `Kubernetes API Server(kube-apiserver)`:提供了HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增删改查等操作的唯一入口，也是集群控制的入口进程。
. `Kubernetes Controller Manager(kube-controller-manager)`:Kubernetes里所有资源对象的自动化控制中心，可以将其理解为资源对象的大总管
. `Kubernetes Scheduler(kube-scheduler)`:负责资源调度（Pod调度）的进程

另外，在Master上通常还需要部署etcd服务，因为Kubernetes里的所有资源对象的数据都被保存在etcd中。

=== Node ===

除了Master，Kubernetes集群中的其他机器被称为Node。Node可以是一台物理机，也可以是一台虚拟机。Node是Kubernetes集群中的工作负载节点，每个Node都会被Master分配一些工作负载（Docker容器），当某个Node宕机时，其上的工作负载会被Master自动转移到其他节点上。

每个Node上都运行着以下关键程序

. kubelet:负责Pod对应的容器的创建、启停等任务，同时与Master密切协作，实现集群管理的基本功能
. kube-proxy:实现Kubernetes Service的通信与负载均衡机制的重要组件
. Docker Engine(docker):Docker引擎，负责本机的容器的创建和管理工作。

Node可已在运行期间动态增加到Kubernetes集群中，前提是这个节点上已经正确安装，配置和启动了上述关键进程，在默认情况下kublet会向Master注册自己，这也是Kubernetes推荐的Node管理方式。一旦Node被纳入集群管理范围，kubelet进程就会定时向Master回报自身的情报，例如操作系统，docker版本，机器的cpu和内存情况，以及当前有哪些pod在运行等，这样Master就可以获知每个node的资源使用情况，并实现高效均衡的资源调度策略。

=== Pod ===

image:pod.png[]

每个pod都有一个特殊的被称为根容器的**Pause**容器。**Pause**容器对应的镜像属于kubernetes平台的一部分，除了pause容器，每个pod还包含一个或多个紧密相关的用户业务容器。

pause容器作为pod的根容器，它的状态代表整个容器组的状态，pod中的多个业务容器共享pause容器的ip，共享pause容器挂接的volume，可以简化密切关联的业务容器之间的通信问题，也很好的解决了他们之间的文件共享问题。

kubernetes为每个pod都分配了唯一的ip地址（pod ip），一个pod中的多个容器共享pod ip，一个pod里的容器可以与另外主机上的pod容器直接通信。

pod有两种类型，普通的pod及静态pod。静态pod并没有被存放在kubernetes的etcd存储里，而是被存放在某个具体的node上的一个具体的文件中，并且只在此node上启动，运行。普通的pod一旦被创建，就会被放入etcd中存储，随后会被kubernetes master调度到某个具体的node上并进行绑定，随后该pod被对应的node上的kubelet进程实例化成一组相关的docker容器并启动。默认情况下，pod中的某个容器停止时，kubernetes会自动检测到这个问题并且重新启动这个pod，如果pod所在的node宕机，就会将这个node上的所有pod重新调度到其他节点上。

image:pod-node.png[]

[source, YAML]
----
# 核心api v1
apiVersion: v1
# 这是一个pod的定义
kind: Pod
metadata:
  name: myweb
  labels:
    name: myweb
# pod里所包含的容器组的定义在spec中声明
spec:
  containers:
  - name: myweb
    image: kubeguide/tomcat-app:v1
    # 资源配额限定
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "64Mi"
        cpu: "250m"
    ports:
    - containerPort: 8080
    env:
    - name: MYSQL_SERVICE_HOST
      value: 'mysql'
    - name: MYSQL_SERVICE_PORT
      value: '3306'
----

NOTE: requests表示该资源的最小申请量，系统必须满足要求，limits表示该资源最大允许使用的量，不能被突破，当容器试图使用超过这个量的资源时，可能会被kubernetes杀掉重启。cpu是相对值，通常一个容器的cpu配额被定义成100-300m，即0.1-0.3个cpu。memory就是内存的字节数。

=== Event ===

Event是一个事件的记录，记录了事件的最早产生时间，最后重现时间，重复次数，发起者，类型，以及导致此事件的原因等众多信息。Event通常会被关联到某个具体的资源对象上，是排故障的重要参考信息， `kubectl describe pod ...` 来查看具体pod的event信息

=== Label ===

一个label是一个key=value的键值对，key和value都由用户自己指定。label可以被附加到各种资源对象上，例如node，pod，service，rc等，一个资源对象可以定义任意数量的label，同一个label可以被添加到任意数量的资源对象上，label通常在资源对象定义时确定，也可以在对象创建后动态添加删除。

我们可以通过label selector（标签选择器）查询和筛选拥有某些label的资源对象。

=== rplication controller ===

简称RC，它定义了一个期望的场景，即声明某种pod的副本数量在任意时刻都符合某个预期值，其定义包括如下几个部分：

. pod期待的副本数量
. 用于筛选目标pod的lable selector
. 当pod的副本数量小于预期数量的时候，用于创建新pod的pod模板

[source,YAML]
----
aptVersion: v1
kind: ReplicationController
metadata:
  name: frontend
spec:
  replicas: 1
  selector:
    tier: frontend
  template:
    metadata:
      labels:
        app: app-demo
        tier: frontend
    spec:
      containers:
      - name: tomcat-demo
        image: tomcat
        imagePullPolicy: IfNotPresent
        env:
        - name: GET_HOSTS_FROM
          value: dns
        ports:
        - containerPort: 80
----

当我们定义了一个RC并将其提交到kubernetes集群中后，master上的controller manager组件就得到通知，定期巡检系统中当前存活的目标pod，并确保目标pod实例的数量刚好等于rc的期望值，如果有过多的pod副本在运行，系统就会停掉一些pod，否则系统会再自动创建一些pod。

kubernetes 1.2中将rplication controller更新为replica set，RS支持集合的label selector。

=== Deployment ===

Deployment内部使用Replica Set来实现目的，无论从Deployment的作用与目的，YAML定义，还是从它的具体命令操作来看，我们都可以把它看作RC的一次升级。

[source,YAML]
----
aptVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 1
  selector:
    matchLabels:
      tier: frontend
    matchExpressions:
      - {key: tier, operator: In, vlaues:[frontend]}
  template:
    metadata:
      labels:
        app: app-demo
        tier: frontend
    spec:
      containers:
      - name: tomcat-demo
        image: tomcat
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
----

=== Horizontal Pod Autoscaler

HPA与之前的RC、Deployment一样，也属于Kubernetes资源对象。通过追踪分析指定RC控制的所有目标Pod的负载变化情况，来确定是否需要有针对性的调整目标Pod的副本数量，当前HPA有以下两种方式作为Pod负载的度量指标：

. CPUUtilizationPercentage
. 应用程序自定义的度量指标，比如服务在每秒内的相应请求数（TPS或QPS）

CPUUtilizationPercentage是一个算数平均值，即目标Pod所有副本自身的CPU利用率的平均值。一个Pod自身的CPU利用率是该Pod当前CPU的使用量除以它的Pod Request的值，比如定义一个Pod的Pod Request为0.4，而当前Pod的CPU使用量为0.2，则他的CPU使用率为50%。如果某一时刻CPUUtilizationPercentage的值超过了80%，则意味着当前Pod副本数量很可能不足以支撑接下来更多的请求，需要进行动态扩容，而在请求高分时段过去后，Pod的CPU利用率又会降下来，此时对应的Pod副本数量应该自动减少到一个合理的水平。如果目标Pod没有定义Pod Request的值，则无法使用CPUUtilizationPercentage实现Pod横向自动扩容。

=== StatefulSet

Pod的管理对象RC、Deployment、DaemonSet和Job都面向无状态的服务。但现实中有很多服务是有状态的，特别是一些复杂的中间件集群，例如MySQL集群，这些应用集群有4个共同点：

. 每个节点都有固定的身份ID，通过这个ID，集群中的成员可以相互发现并通信
. 集群的规模比较固定，集群规模不能随意变动
. 集群中每个节点都是有状态的，通常会持久化数据到永久存储中
. 如果磁盘损坏，则集群里的某个节点无法正常运行，集群功能受损

StatefulSet有如下特性：

. StatefulSet里的每个Pod都有稳定、唯一的网络标识，可以用来发现集群内的其他成员。假设StatefulSet的名称为kafka，那么第一个Pod叫kafka-0，第2个叫kafka-1
. StatefulSet控制的Pod副本的起停顺序是受控制的，操作第n个Pod时，前n-1个Pod已经是运行且准备好的状态
. StatefulSet里的Pod采用稳定的持久化存储卷，通过PV或PVC来实现，删除Pod时默认不会删除与StatefulSet相关的存储卷

StatefulSet除了要与PV卷捆绑使用以存储Pod的数据状态，还要与Headless Service配合使用，即在每个StatefulSet定义中都要声明它属于那个Headless Service，Headless Service没有Cluster IP，如果解析Headless Service的DNS域名，则返回的是该Service对应的全局Pod的Endpoint列表。StatefulSet在Headless Service的基础上又为StatefulSet控制的每个Pod实例都创建了一个DNS域名，这个域名的格式为： `$(podname).$(headless service name)`。比如一个3节点的Kafka的StatefulSet集群对应的Headless Service的名称为kafka，StatefulSet的名称为kafka，则StatefulSet里的3个Pod的DNS名称分别为kafka-0.kafka、kafka-1.kafka、kafka-2.kafka，这些DNS名称可以直接在集群的配置文件中固定下来

=== Service

kubernetes里的每个Service其实就是我们经常提起的微服务架构中的一个微服务。

image::Snipaste_2021-11-20_11-29-18.png[]

Service定义了一个微服务的访问入口地址，前端的应用Pod通过这个入口地址访问其背后的一组由Pod副本组成的集群实例，Service与其后端Pod副本集群之间则是通过Label Selector来实现无缝对接的。RC的作用实际上是保证Service的服务能力和服务质量始终符合预期标准。

每个Node上会有一个kube-proxy进程，本质是一个智能的软件负载均衡器，负责把对Service的请求转发到后端的某个Pod实例上，并在内部实现服务的负载均衡与会话保持机制。但是Kubernetes发明了一种很巧妙的设计：Service没有共用一个负载均衡器的IP地址，每个Service都被分配了一个全局唯一的虚拟IP地址，这个虚拟IP被成为Cluster IP，这样一来，每个服务就变成了具备唯一IP地址的通信节点，服务调用就变成了最基础的TCP网络通信问题。

当一个Pod销毁和重新创建的时候Pod的IP地址与之前旧Pod不同，而Service一旦被创建，Kubernetes就会自动为它分配一个可用的Cluster IP，而且在Service的整个生命周期内，它的Cluster IP不会发生改变，但是Kubernetes用Service的Name与Service的Cluster IP地址做了一个DNS域名映射，解决了IP地址变更的问题。

[source, yaml]
.tomcat-server.yaml
----
apiVersion: v1
kind: Service
metadata:
  name: tomcat-service
spec:
  ports:
  - port: 8080
  selector:
    tier: frontend
----

上述内容定义了一个名为tomcat-service的Service，它的服务端口为8080，拥有 `tier: frontend` 的所有Pod实例都属于它，运行以下命令进行创建： `kubectl create -f tomcat-server.yaml` 

很多服务都存在多个端口的问题，通常一个端口提供业务服务，另外一个端口提供管理服务，Service支持多个Endpoint，在存在多个Endpoint的情况下，要求每个Endpoint都定义一个名称来区分。例如

[source, yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: tomcat-service
spec:
  ports:
  - port:8080
    name: service-port
  -port: 8005
    name: shutdown-port
  selector:
    tier: frontend
----

==== 外部系统访问Service的问题

为了更深入的理解和掌握Kubernetes，我们需要弄明白Kubernetes里的3种IP：

. Node IP：Node的IP地址
. Pod IP：Pod的IP地址
. Cluster IP：Service的IP地址

首先，Node IP是Kubernetes集群种每个节点的物理网卡的IP地址，是一个真实存在的物理网络，所有属于这个网络的服务器都能通过这个网络直接通信，不管其中是否有部分节点不属于这个kubernetes集群。这也表明在kubernetes集群之外的节点访问kubernetes集群之内的某个节点或者TCP/IP服务时，都必须通过Node IP通信。

Pod IP是每个Pod的IP地址，他是Docker Engine根据docker0网桥的IP地址段进行分配的，通常是一个虚拟的二层网络，kubernetes里一个Pod的容器访问另外一个Pod里的容器时，就是通过Pod IP所在的虚拟二层网络进行通信的，而真实的TCP/IP流量时通过Node IP所在的物理网卡流出的

Cluster IP是一种虚拟的IP，但更像一个伪造的IP网络，因为：

. Cluster IP仅仅作用于kubernetes Service这个对象，并由kubernetes管理和分配
. Cluster IP无法被Ping，因为没有一个实体网络对象来响应
. Cluster IP只能结合Service Port组成一个具体的通信端口，单独的Cluster IP不具备TCP/IP通信的基础，并且他们属于kubernetes集群这样一个封闭的空间，集群外的节点如果要访问这个通信端口，则需要做一些额外的工作
. kubernetes集群内，Node IP网络，Pod IP网络与Cluster IP网络之间的通信，采用的是kubernetes自己设计的一种编程方式的特殊路由规则，与我们熟知的IP路由有很大的不同

那要如何实现外部应用访问集群内部的服务模块，可以使用NodePort

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: tomcat-service
spec:
  type: NodePort
  ports:
  - port:8080
    nodePort: 31002
  selector:
    tier: frontend
----

其中，nodePort:31002这个属性表明手动指定tomcat-service的NodePort为31002，否则Kubernetes会自动分配一个可用的端口。

NodePort的实现方式是在Kubernetes集群里的每个Node上都为需要外部访问的Service开启一个对应的TCP监听端口，外部系统只要用任意一个Node的IP地址+具体的NodePort端口号即可以访问此服务，在任意Node上运行netstat命令，就可以看到有NodePort端口被监听。

=== Job

批处理任务通常并行（或者串行）启动多个计算进程去处理一批工作项（work item），在处理完成后，整个批处理任务结束。Job也是一组Pod容器，但是Job控制Pod副本与RC等控制器的工作机制有以下重要差别

. Job所控制的Pod副本是短暂运行的，可以将其视为一组Docker容器，其中的每个Docker容器都仅仅运行一次。当Job控制的所有Pod副本都运行结束时，对应的Job也就结束了。Job在实现方式上与RC等副本控制器不同，Job生成Pod副本时不能自动重启的，对应Pod副本的RestartPoliy都被设置为Never。CronJob提供了类似crontab的定时任务，解决了某些批处理任务需要定时反复执行的问题
. Job所控制的Pod副本的工作模式能够多实例并行计算，以TensorFlow框架为例，可以将一个机器学习的计算任务分不到10台机器上，在每台机器上都运行一个worker执行计算任务，这很适合通过Job生成10个Pod副本同事启动运算。

=== Volume

存储卷是Pod中能够被多个容器访问的共享目录。Kubernetes的Volume概念、用途和目的与Docker的Volume比较类似，但两者不能等价。首先Kubernetes中的Volume被定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下；其次，Kubernetes中的Volume与Pod的生命周期相同，但与容器的生命周期不相关，当容器终止或重启时，Volume中的数据也不会丢失。最后，Kubernetes支持多种类型的Volume，例如GlusterFS、Ceph等先进的分布式文件系统。

Volume的使用也比较简单，在大多数情况下，我们先在Pod上声明一个Volume，然后在容器里引用该Volume并挂载（Mount）到容器里的某个目录上。举例来说，我们要给之前的Tomcat Pod增加一个名为datavol的Volume，并且挂载到容器的 `/mydata-data` 目录上，则只要对Pod的定义文件做如下修正即可

[source, yaml]
----
template:
  metadata:
    labels:
      app: app-demo
      tier: frontend
  spec:
    volumes:
    - name: datavol
      emptyDir: {}
    containers:
    - name: tomcat-demo
      image: tomcat
      volumeMounts:
      - mountPath: /mydata-data
        name: datavol
      imagePullPolicy: IfNotPresent
----

Kubernetes提供了非常丰富的Volume类型，下面逐一进行说明：

. emptyDir：一个emptyDir Volume是在Pod分配到Node时创建的。从它的名称就可以看出，他的初始内容为空，并且无须指定宿主机上对应的目录文件，因为这是Kubernetes自动分配的一个目录，当Pod从Node上移除时，emptyDir中的数据也会被永久删除。emptyDir的一些用途如下：临时空间；长时间任务的中间过程CheckPoint的临时保存目录；一个容器需要从另一个容器中获取数据的目录。
. hostPath：hostPath为在Pod上挂载宿主机上的文件或目录，它通常可以用于以下几个方面：1，容器应用程序生成的日志文件需要永久保存时，可以使用宿主机的高速文件系统进行存储；2，需要访问宿主机上Docker引擎内部数据结构的容器应用时，可以通过定义hostPath为宿主机 `/var/lib/docker` 目录，使容器内部应用可以直接访问Docker的文件系统。在使用这种类型的Volume时，需要注意以下几点：1，在不同的Node上具有相同配置的Pod，可能会因为宿主机上目录和文件不同而导致Volume上目录和文件的访问结果不一致；2，如果使用了资源配额管理，则Kubernetes无法将hostPath在宿主机上使用的资源纳入管理。
+
[source, yaml]
----
volumes:
- name: "persistent-storage"
  hostPath:
    path: "/data"
----
. gcePersistentDisk：使用这种类型的Volume表示使用谷歌公有云提供的永久磁盘（Persistent Disk，PD）存放Volume的数据，它与emptyDir不同，PD上的内容会被永久保存，当Pod被删除时，PD只是被卸载（Unmount），但不会被删除。
+
[sourc,yaml]
----
volumes:
- name: test-volume
  gcePersistentDisk:
    pdName: my-data-disk
    fsType: ext4
----
. awsElasticBlockStore：亚马逊公有云提供的EBS Volume存储数据
+
[sourc,yaml]
----
volumes:
- name: test-volume
  awsElasticBlockStore:
    volumeID: aws://<availability-zone>/<volume-id>
    fsType: ext4
----
. NFS：使用NFS网络文件系统提供的共享目录存储数据时，我们需要在系统中部署一个NFS Server。
+
[sourc,yaml]
----
volumes:
- name: nfs
  nfs:
    server: nfs-server.localhost
    path: "/"
----
. 其他类型的Volume：
.. iscis：使用iSCSI存储设备上的目录挂载到Pod中
.. flocker：使用Flocker管理存储卷
.. glusterfs：使用开源GlusterFS网络文件系统的目录挂载到Pod中
.. rbd：使用Ceph块设备共享存储（Rados Block Device）挂载到Pod中
.. gitRepo：通过挂载一个空目录，并从Git库中clone一个仓库以供Pod使用
.. secret： 一个Secret Volume用于为Pod提供加密的信息，你可以将定义在Kubernetes中的Secret直接挂载为文件让Pod访问。Secret Volume是通过TMFS（内存文件系统）实现的，这种类型的Volume是不会被持久化的

=== Persistent Volume

之前提到的Volume是被定义在Pod上的，属于计算资源的一部分，而实际上，网络存储是相对独立于计算资源而存在的一种实体资源。比如在使用虚拟机的情况下，我们通常会先定义一个网络存储，然后从中划出一个网盘并挂接到虚拟机上。Persistent Volume（PV）和与之相关联的Persistent Volume Claim（PVC）也起到了类似的作用

PV可以被理解成Kubernetes集群中某个网络存储对应的一块存储，它与Volume类似，但有以下区别

. PV只能是网络存储，不属于任何Node，但可以在每个Node上访问
. PV并不是被定义在Pod上的，而是独立与Pod之外定义的
. PV目前支持的类型包括：gcePersistentDisk、awsElasticBlockStore、AzureFile、AzureDisk、FC（Fibre Channel）、Flockers、NFS、iSCSI、RBD（Rados Block Device）、CephFS、Cinder、GlusterFS、VsphereVolume、Quobyte Volumes、VMware Photon、Portworx Volumes、ScaleIO Volumes

下面给出了NFS类型的PV的一个yaml定义文件，声明了需要5Gi的存储空间

----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv003
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    path: /somepath
    server: 172.17.0.2
----

比较重要的是PV的 `accessModes` 属性，目前有以下类型：

 . ReadWriteOnce：读写权限，并且只能被单个Node挂载
 . ReadOnlyMany：只读权限，允许被多个Node挂载
 . ReadWriteMany：读写权限，允许被多个Node挂载
 
如果某个Pod想申请某种类型的PV，则首先需要定义一个PersistentVolumeClain对象

[source,yaml]
----
kind: PersistentVolumeClain
apiVersion: v1
metadata:
  name: myclain
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 8Gi
----

然后，在Pod的Volume定义中引用上述PVC即可

[source, yaml]
----
volumes:
  -name: mypd
    persistentVolumeClain:
      clainName: myclaim
----

最后说说PV的状态，PV是有状态的对象，它的状态有以下几种：

- Available：空闲状态
- Bound：已经绑定到某个PVC上
- Released：对应的PVC已经被删除，但资源还没有被集群收回
- Failed：PV自动回收失败

=== Namespace

Namespace（命名空间）是Kubernetes系统中的另一个非常重要的概念，Namespace在很多情况下用于实现多租户的资源隔离。Namespace通过将集群内部的资源对象分配到不同的Namespace中，形成逻辑上分组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源的同事还能被分别管理

Kubernetes集群在启动后会创建一个 `default` 的Namespace，通过kubectl可以查看： `kubectl get namespaces`

Namespace的定义很简单，如下所示的yaml定义了名为 `development` 的 Namespace

[source, yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: development
----

一旦创建了Namespace，我们在创建资源对象时就可以指定这个资源对象属于那个Namespace。

[source, yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: development
spec:
  containers:
  - image: busybox
    command:
    - sleep
    - "3600"
  name: busybox
----

此时查看对应Namespace的Pod：`kubectl get pods --namespace=development`

=== Annotation

Annotation（注解）与Label类似，也使用key/value键值对的形式进行定义。不同的是Label具有严格的命名规则，它定义的是Kubernetes对象的元数据（Metadata），并且用户Label Selector。Annotation则是用户任意定义的附加信息，以便于外部工具查找。在很多时候，Kubernetes的模块自身会通过Annotation标记资源对象的一些特殊信息。

通常来说，用Annotation来记录的信息如下：

- build信息，release信息，Docker镜像信息，例如时间戳、release id号、PR号，镜像Hash值
- 日志库、监控库、分析库等资源库的地址信息
- 程序调试工具信息，例如工具名称、版本号等
- 团队的联系信息，例如电话号码、负责人名称、网址等

=== ConfigMap

为了集中管理系统的配置参数，而不是管理一堆配置文件。Kubernetes把所有的配置项都当作 `key-value` 字符串，当然value可以来自某个文本文件。这些配置项可以作为Map表中的一个项，整个Map的数据可以被持久化存储在Kubernetes的Etcd数据库中，然后提供API以方便Kubernetes相关组件或客户应用CRUD操作这些数据，上述专门用来保存配置参数的Map就是Kubernetes ConfigMap资源对象。

接下里Kubernetes提供了一种内建机制，将存储在etcd中的ConfigMap通过Volume映射的方式变成目标Pod内的配置文件，不管目标Pod被调度到哪台服务器上，都会完成自动映射。进一步地，如果ConfigMap中的key-value数据被修改，则映射到Pod中的配置文件也会随之更新。






























