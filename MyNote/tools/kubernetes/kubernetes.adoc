= Kubernetes权威指南 =
Korov9 <korov9@163.com>
v1.0 2021-02-24
:imagesdir: picture
:toc: right

== Kubernetes的基本概念和术语 ==

Kubernetes中的大部分概念如Node,Pod,Replication Controller,Service等都可以被看作一种资源对象，几乎所有资源对象都可以通过Kubernetes提供的kubectl工具执行增删改查等操作并将其保存在etcd中持久化存储。从这个角度来看，Kubernetes其实是一个高度自动化的资源控制系统，他通过跟踪对比etcd库里保存的“资源期望状态”与当前环境中的“实际资源状态”的差异来实现自动控制和自动纠错的高级功能。

Kubernetes平台采用了**核心+外围扩展**的设计思路，在保持平台核心稳定的同时具备持续演进升级的优势。Kubernetes大部分常见的核心资源对象都归属于 `v1` 这个核心 `API` ，比如 `Node,Pod,Service,Endpoints,Namespace,RC,PersistenVolumn` 等。在 `1.9` 版本之后引入了 `apps/v1` 这个正式的扩展 `API` 组。

=== Master ===

Kubernetes里的Master指的是集群控制节点，在每个Kubernetes集群里都需要有一个Master来负责整个集群的管理和控制，基本上Kubernetes的所有控制命令都发给它，它负责具体的执行过程。Master通常会占据一个独立的服务器（高可用部署建议用3台服务器）。

在Master上运行这以下关键进程：

. `Kubernetes API Server(kube-apiserver)`:提供了HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增删改查等操作的唯一入口，也是集群控制的入口进程。
. `Kubernetes Controller Manager(kube-controller-manager)`:Kubernetes里所有资源对象的自动化控制中心，可以将其理解为资源对象的大总管
. `Kubernetes Scheduler(kube-scheduler)`:负责资源调度（Pod调度）的进程

另外，在Master上通常还需要部署etcd服务，因为Kubernetes里的所有资源对象的数据都被保存在etcd中。

=== Node ===

除了Master，Kubernetes集群中的其他机器被称为Node。Node可以是一台物理机，也可以是一台虚拟机。Node是Kubernetes集群中的工作负载节点，每个Node都会被Master分配一些工作负载（Docker容器），当某个Node宕机时，其上的工作负载会被Master自动转移到其他节点上。

每个Node上都运行着以下关键程序

. kubelet:负责Pod对应的容器的创建、启停等任务，同时与Master密切协作，实现集群管理的基本功能
. kube-proxy:实现Kubernetes Service的通信与负载均衡机制的重要组件
. Docker Engine(docker):Docker引擎，负责本机的容器的创建和管理工作。

Node可已在运行期间动态增加到Kubernetes集群中，前提是这个节点上已经正确安装，配置和启动了上述关键进程，在默认情况下kublet会向Master注册自己，这也是Kubernetes推荐的Node管理方式。一旦Node被纳入集群管理范围，kubelet进程就会定时向Master回报自身的情报，例如操作系统，docker版本，机器的cpu和内存情况，以及当前有哪些pod在运行等，这样Master就可以获知每个node的资源使用情况，并实现高效均衡的资源调度策略。

=== Pod ===

image:pod.png[]

每个pod都有一个特殊的被称为根容器的**Pause**容器。**Pause**容器对应的镜像属于kubernetes平台的一部分，除了pause容器，每个pod还包含一个或多个紧密相关的用户业务容器。

pause容器作为pod的根容器，它的状态代表整个容器组的状态，pod中的多个业务容器共享pause容器的ip，共享pause容器挂接的volume，可以简化密切关联的业务容器之间的通信问题，也很好的解决了他们之间的文件共享问题。

kubernetes为每个pod都分配了唯一的ip地址（pod ip），一个pod中的多个容器共享pod ip，一个pod里的容器可以与另外主机上的pod容器直接通信。

pod有两种类型，普通的pod及静态pod。静态pod并没有被存放在kubernetes的etcd存储里，而是被存放在某个具体的node上的一个具体的文件中，并且只在此node上启动，运行。普通的pod一旦被创建，就会被放入etcd中存储，随后会被kubernetes master调度到某个具体的node上并进行绑定，随后该pod被对应的node上的kubelet进程实例化成一组相关的docker容器并启动。默认情况下，pod中的某个容器停止时，kubernetes会自动检测到这个问题并且重新启动这个pod，如果pod所在的node宕机，就会将这个node上的所有pod重新调度到其他节点上。

image:pod-node.png[]

[source, YAML]
----
# 核心api v1
apiVersion: v1
# 这是一个pod的定义
kind: Pod
metadata:
  name: myweb
  labels:
    name: myweb
# pod里所包含的容器组的定义在spec中声明
spec:
  containers:
  - name: myweb
    image: kubeguide/tomcat-app:v1
    # 资源配额限定
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "64Mi"
        cpu: "250m"
    ports:
    - containerPort: 8080
    env:
    - name: MYSQL_SERVICE_HOST
      value: 'mysql'
    - name: MYSQL_SERVICE_PORT
      value: '3306'
----

NOTE: requests表示该资源的最小申请量，系统必须满足要求，limits表示该资源最大允许使用的量，不能被突破，当容器试图使用超过这个量的资源时，可能会被kubernetes杀掉重启。cpu是相对值，通常一个容器的cpu配额被定义成100-300m，即0.1-0.3个cpu。memory就是内存的字节数。

=== Event ===

Event是一个事件的记录，记录了事件的最早产生时间，最后重现时间，重复次数，发起者，类型，以及导致此事件的原因等众多信息。Event通常会被关联到某个具体的资源对象上，是排故障的重要参考信息， `kubectl describe pod ...` 来查看具体pod的event信息

=== Label ===

一个label是一个key=value的键值对，key和value都由用户自己指定。label可以被附加到各种资源对象上，例如node，pod，service，rc等，一个资源对象可以定义任意数量的label，同一个label可以被添加到任意数量的资源对象上，label通常在资源对象定义时确定，也可以在对象创建后动态添加删除。

我们可以通过label selector（标签选择器）查询和筛选拥有某些label的资源对象。

=== rplication controller ===

简称RC，它定义了一个期望的场景，即声明某种pod的副本数量在任意时刻都符合某个预期值，其定义包括如下几个部分：

. pod期待的副本数量
. 用于筛选目标pod的lable selector
. 当pod的副本数量小于预期数量的时候，用于创建新pod的pod模板

[source,YAML]
----
aptVersion: v1
kind: ReplicationController
metadata:
  name: frontend
spec:
  replicas: 1
  selector:
    tier: frontend
  template:
    metadata:
      labels:
        app: app-demo
        tier: frontend
    spec:
      containers:
      - name: tomcat-demo
        image: tomcat
        imagePullPolicy: IfNotPresent
        env:
        - name: GET_HOSTS_FROM
          value: dns
        ports:
        - containerPort: 80
----

当我们定义了一个RC并将其提交到kubernetes集群中后，master上的controller manager组件就得到通知，定期巡检系统中当前存活的目标pod，并确保目标pod实例的数量刚好等于rc的期望值，如果有过多的pod副本在运行，系统就会停掉一些pod，否则系统会再自动创建一些pod。

kubernetes 1.2中将rplication controller更新为replica set，RS支持集合的label selector。

=== Deployment ===

Deployment内部使用Replica Set来实现目的，无论从Deployment的作用与目的，YAML定义，还是从它的具体命令操作来看，我们都可以把它看作RC的一次升级。

[source,YAML]
----
aptVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 1
  selector:
    matchLabels:
      tier: frontend
    matchExpressions:
      - {key: tier, operator: In, vlaues:[frontend]}
  template:
    metadata:
      labels:
        app: app-demo
        tier: frontend
    spec:
      containers:
      - name: tomcat-demo
        image: tomcat
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
----

=== Horizontal Pod Autoscaler

HPA与之前的RC、Deployment一样，也属于Kubernetes资源对象。通过追踪分析指定RC控制的所有目标Pod的负载变化情况，来确定是否需要有针对性的调整目标Pod的副本数量，当前HPA有以下两种方式作为Pod负载的度量指标：

. CPUUtilizationPercentage
. 应用程序自定义的度量指标，比如服务在每秒内的相应请求数（TPS或QPS）

CPUUtilizationPercentage是一个算数平均值，即目标Pod所有副本自身的CPU利用率的平均值。一个Pod自身的CPU利用率是该Pod当前CPU的使用量除以它的Pod Request的值，比如定义一个Pod的Pod Request为0.4，而当前Pod的CPU使用量为0.2，则他的CPU使用率为50%。如果某一时刻CPUUtilizationPercentage的值超过了80%，则意味着当前Pod副本数量很可能不足以支撑接下来更多的请求，需要进行动态扩容，而在请求高分时段过去后，Pod的CPU利用率又会降下来，此时对应的Pod副本数量应该自动减少到一个合理的水平。如果目标Pod没有定义Pod Request的值，则无法使用CPUUtilizationPercentage实现Pod横向自动扩容。

=== StatefulSet

Pod的管理对象RC、Deployment、DaemonSet和Job都面向无状态的服务。但现实中有很多服务是有状态的，特别是一些复杂的中间件集群，例如MySQL集群，这些应用集群有4个共同点：

. 每个节点都有固定的身份ID，通过这个ID，集群中的成员可以相互发现并通信
. 集群的规模比较固定，集群规模不能随意变动
. 集群中每个节点都是有状态的，通常会持久化数据到永久存储中
. 如果磁盘损坏，则集群里的某个节点无法正常运行，集群功能受损

StatefulSet有如下特性：

. StatefulSet里的每个Pod都有稳定、唯一的网络标识，可以用来发现集群内的其他成员。假设StatefulSet的名称为kafka，那么第一个Pod叫kafka-0，第2个叫kafka-1
. StatefulSet控制的Pod副本的起停顺序是受控制的，操作第n个Pod时，前n-1个Pod已经是运行且准备好的状态
. StatefulSet里的Pod采用稳定的持久化存储卷，通过PV或PVC来实现，删除Pod时默认不会删除与StatefulSet相关的存储卷

StatefulSet除了要与PV卷捆绑使用以存储Pod的数据状态，还要与Headless Service配合使用，即在每个StatefulSet定义中都要声明它属于那个Headless Service，Headless Service没有Cluster IP，如果解析Headless Service的DNS域名，则返回的是该Service对应的全局Pod的Endpoint列表。StatefulSet在Headless Service的基础上又为StatefulSet控制的每个Pod实例都创建了一个DNS域名，这个域名的格式为： `$(podname).$(headless service name)`。比如一个3节点的Kafka的StatefulSet集群对应的Headless Service的名称为kafka，StatefulSet的名称为kafka，则StatefulSet里的3个Pod的DNS名称分别为kafka-0.kafka、kafka-1.kafka、kafka-2.kafka，这些DNS名称可以直接在集群的配置文件中固定下来

=== Service

kubernetes里的每个Service其实就是我们经常提起的微服务架构中的一个微服务。

image::Snipaste_2021-11-20_11-29-18.png[]

Service定义了一个微服务的访问入口地址，前端的应用Pod通过这个入口地址访问其背后的一组由Pod副本组成的集群实例，Service与其后端Pod副本集群之间则是通过Label Selector来实现无缝对接的。RC的作用实际上是保证Service的服务能力和服务质量始终符合预期标准。

每个Node上会有一个kube-proxy进程，本质是一个智能的软件负载均衡器，负责把对Service的请求转发到后端的某个Pod实例上，并在内部实现服务的负载均衡与会话保持机制。但是Kubernetes发明了一种很巧妙的设计：Service没有共用一个负载均衡器的IP地址，每个Service都被分配了一个全局唯一的虚拟IP地址，这个虚拟IP被成为Cluster IP，这样一来，每个服务就变成了具备唯一IP地址的通信节点，服务调用就变成了最基础的TCP网络通信问题。

当一个Pod销毁和重新创建的时候Pod的IP地址与之前旧Pod不同，而Service一旦被创建，Kubernetes就会自动为它分配一个可用的Cluster IP，而且在Service的整个生命周期内，它的Cluster IP不会发生改变，但是Kubernetes用Service的Name与Service的Cluster IP地址做了一个DNS域名映射，解决了IP地址变更的问题。

[source, yaml]
.tomcat-server.yaml
----
apiVersion: v1
kind: Service
metadata:
  name: tomcat-service
spec:
  ports:
  - port: 8080
  selector:
    tier: frontend
----

上述内容定义了一个名为tomcat-service的Service，它的服务端口为8080，拥有 `tier: frontend` 的所有Pod实例都属于它，运行以下命令进行创建： `kubectl create -f tomcat-server.yaml` 

很多服务都存在多个端口的问题，通常一个端口提供业务服务，另外一个端口提供管理服务，Service支持多个Endpoint，在存在多个Endpoint的情况下，要求每个Endpoint都定义一个名称来区分。例如

[source, yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: tomcat-service
spec:
  ports:
  - port:8080
    name: service-port
  -port: 8005
    name: shutdown-port
  selector:
    tier: frontend
----

==== 外部系统访问Service的问题

为了更深入的理解和掌握Kubernetes，我们需要弄明白Kubernetes里的3种IP：

. Node IP：Node的IP地址
. Pod IP：Pod的IP地址
. Cluster IP：Service的IP地址

首先，Node IP是Kubernetes集群种每个节点的物理网卡的IP地址，是一个真实存在的物理网络，所有属于这个网络的服务器都能通过这个网络直接通信，不管其中是否有部分节点不属于这个kubernetes集群。这也表明在kubernetes集群之外的节点访问kubernetes集群之内的某个节点或者TCP/IP服务时，都必须通过Node IP通信。

Pod IP是每个Pod的IP地址，他是Docker Engine根据docker0网桥的IP地址段进行分配的，通常是一个虚拟的二层网络，kubernetes里一个Pod的容器访问另外一个Pod里的容器时，就是通过Pod IP所在的虚拟二层网络进行通信的，而真实的TCP/IP流量时通过Node IP所在的物理网卡流出的

Cluster IP是一种虚拟的IP，但更像一个伪造的IP网络，因为：

. Cluster IP仅仅作用于kubernetes Service这个对象，并由kubernetes管理和分配
. Cluster IP无法被Ping，因为没有一个实体网络对象来响应
. Cluster IP只能结合Service Port组成一个具体的通信端口，单独的Cluster IP不具备TCP/IP通信的基础，并且他们属于kubernetes集群这样一个封闭的空间，集群外的节点如果要访问这个通信端口，则需要做一些额外的工作
. kubernetes集群内，Node IP网络，Pod IP网络与Cluster IP网络之间的通信，采用的是kubernetes自己设计的一种编程方式的特殊路由规则，与我们熟知的IP路由有很大的不同

那要如何实现外部应用访问集群内部的服务模块，可以使用NodePort

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: tomcat-service
spec:
  type: NodePort
  ports:
  - port:8080
    nodePort: 31002
  selector:
    tier: frontend
----

其中，nodePort:31002这个属性表明手动指定tomcat-service的NodePort为31002，否则Kubernetes会自动分配一个可用的端口。

NodePort的实现方式是在Kubernetes集群里的每个Node上都为需要外部访问的Service开启一个对应的TCP监听端口，外部系统只要用任意一个Node的IP地址+具体的NodePort端口号即可以访问此服务，在任意Node上运行netstat命令，就可以看到有NodePort端口被监听。

=== Job

批处理任务通常并行（或者串行）启动多个计算进程去处理一批工作项（work item），在处理完成后，整个批处理任务结束。Job也是一组Pod容器，但是Job控制Pod副本与RC等控制器的工作机制有以下重要差别

. Job所控制的Pod副本是短暂运行的，可以将其视为一组Docker容器，其中的每个Docker容器都仅仅运行一次。当Job控制的所有Pod副本都运行结束时，对应的Job也就结束了。Job在实现方式上与RC等副本控制器不同，Job生成Pod副本时不能自动重启的，对应Pod副本的RestartPoliy都被设置为Never。CronJob提供了类似crontab的定时任务，解决了某些批处理任务需要定时反复执行的问题
. Job所控制的Pod副本的工作模式能够多实例并行计算，以TensorFlow框架为例，可以将一个机器学习的计算任务分不到10台机器上，在每台机器上都运行一个worker执行计算任务，这很适合通过Job生成10个Pod副本同事启动运算。

=== Volume

存储卷是Pod中能够被多个容器访问的共享目录。Kubernetes的Volume概念、用途和目的与Docker的Volume比较类似，但两者不能等价。首先Kubernetes中的Volume被定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下；其次，Kubernetes中的Volume与Pod的生命周期相同，但与容器的生命周期不相关，当容器终止或重启时，Volume中的数据也不会丢失。最后，Kubernetes支持多种类型的Volume，例如GlusterFS、Ceph等先进的分布式文件系统。

Volume的使用也比较简单，在大多数情况下，我们先在Pod上声明一个Volume，然后在容器里引用该Volume并挂载（Mount）到容器里的某个目录上。举例来说，我们要给之前的Tomcat Pod增加一个名为datavol的Volume，并且挂载到容器的 `/mydata-data` 目录上，则只要对Pod的定义文件做如下修正即可

[source, yaml]
----
template:
  metadata:
    labels:
      app: app-demo
      tier: frontend
  spec:
    volumes:
    - name: datavol
      emptyDir: {}
    containers:
    - name: tomcat-demo
      image: tomcat
      volumeMounts:
      - mountPath: /mydata-data
        name: datavol
      imagePullPolicy: IfNotPresent
----

Kubernetes提供了非常丰富的Volume类型，下面逐一进行说明：

. emptyDir：一个emptyDir Volume是在Pod分配到Node时创建的。从它的名称就可以看出，他的初始内容为空，并且无须指定宿主机上对应的目录文件，因为这是Kubernetes自动分配的一个目录，当Pod从Node上移除时，emptyDir中的数据也会被永久删除。emptyDir的一些用途如下：临时空间；长时间任务的中间过程CheckPoint的临时保存目录；一个容器需要从另一个容器中获取数据的目录。
. hostPath：hostPath为在Pod上挂载宿主机上的文件或目录，它通常可以用于以下几个方面：1，容器应用程序生成的日志文件需要永久保存时，可以使用宿主机的高速文件系统进行存储；2，需要访问宿主机上Docker引擎内部数据结构的容器应用时，可以通过定义hostPath为宿主机 `/var/lib/docker` 目录，使容器内部应用可以直接访问Docker的文件系统。在使用这种类型的Volume时，需要注意以下几点：1，在不同的Node上具有相同配置的Pod，可能会因为宿主机上目录和文件不同而导致Volume上目录和文件的访问结果不一致；2，如果使用了资源配额管理，则Kubernetes无法将hostPath在宿主机上使用的资源纳入管理。
+
[source, yaml]
----
volumes:
- name: "persistent-storage"
  hostPath:
    path: "/data"
----
. gcePersistentDisk：使用这种类型的Volume表示使用谷歌公有云提供的永久磁盘（Persistent Disk，PD）存放Volume的数据，它与emptyDir不同，PD上的内容会被永久保存，当Pod被删除时，PD只是被卸载（Unmount），但不会被删除。
+
[sourc,yaml]
----
volumes:
- name: test-volume
  gcePersistentDisk:
    pdName: my-data-disk
    fsType: ext4
----
. awsElasticBlockStore：亚马逊公有云提供的EBS Volume存储数据
+
[sourc,yaml]
----
volumes:
- name: test-volume
  awsElasticBlockStore:
    volumeID: aws://<availability-zone>/<volume-id>
    fsType: ext4
----
. NFS：使用NFS网络文件系统提供的共享目录存储数据时，我们需要在系统中部署一个NFS Server。
+
[sourc,yaml]
----
volumes:
- name: nfs
  nfs:
    server: nfs-server.localhost
    path: "/"
----
. 其他类型的Volume：
.. iscis：使用iSCSI存储设备上的目录挂载到Pod中
.. flocker：使用Flocker管理存储卷
.. glusterfs：使用开源GlusterFS网络文件系统的目录挂载到Pod中
.. rbd：使用Ceph块设备共享存储（Rados Block Device）挂载到Pod中
.. gitRepo：通过挂载一个空目录，并从Git库中clone一个仓库以供Pod使用
.. secret： 一个Secret Volume用于为Pod提供加密的信息，你可以将定义在Kubernetes中的Secret直接挂载为文件让Pod访问。Secret Volume是通过TMFS（内存文件系统）实现的，这种类型的Volume是不会被持久化的

=== Persistent Volume

之前提到的Volume是被定义在Pod上的，属于计算资源的一部分，而实际上，网络存储是相对独立于计算资源而存在的一种实体资源。比如在使用虚拟机的情况下，我们通常会先定义一个网络存储，然后从中划出一个网盘并挂接到虚拟机上。Persistent Volume（PV）和与之相关联的Persistent Volume Claim（PVC）也起到了类似的作用

PV可以被理解成Kubernetes集群中某个网络存储对应的一块存储，它与Volume类似，但有以下区别

. PV只能是网络存储，不属于任何Node，但可以在每个Node上访问
. PV并不是被定义在Pod上的，而是独立与Pod之外定义的
. PV目前支持的类型包括：gcePersistentDisk、awsElasticBlockStore、AzureFile、AzureDisk、FC（Fibre Channel）、Flockers、NFS、iSCSI、RBD（Rados Block Device）、CephFS、Cinder、GlusterFS、VsphereVolume、Quobyte Volumes、VMware Photon、Portworx Volumes、ScaleIO Volumes

下面给出了NFS类型的PV的一个yaml定义文件，声明了需要5Gi的存储空间

----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv003
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    path: /somepath
    server: 172.17.0.2
----

比较重要的是PV的 `accessModes` 属性，目前有以下类型：

 . ReadWriteOnce：读写权限，并且只能被单个Node挂载
 . ReadOnlyMany：只读权限，允许被多个Node挂载
 . ReadWriteMany：读写权限，允许被多个Node挂载
 
如果某个Pod想申请某种类型的PV，则首先需要定义一个PersistentVolumeClain对象

[source,yaml]
----
kind: PersistentVolumeClain
apiVersion: v1
metadata:
  name: myclain
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 8Gi
----

然后，在Pod的Volume定义中引用上述PVC即可

[source, yaml]
----
volumes:
  -name: mypd
    persistentVolumeClain:
      clainName: myclaim
----

最后说说PV的状态，PV是有状态的对象，它的状态有以下几种：

- Available：空闲状态
- Bound：已经绑定到某个PVC上
- Released：对应的PVC已经被删除，但资源还没有被集群收回
- Failed：PV自动回收失败

=== Namespace

Namespace（命名空间）是Kubernetes系统中的另一个非常重要的概念，Namespace在很多情况下用于实现多租户的资源隔离。Namespace通过将集群内部的资源对象分配到不同的Namespace中，形成逻辑上分组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源的同事还能被分别管理

Kubernetes集群在启动后会创建一个 `default` 的Namespace，通过kubectl可以查看： `kubectl get namespaces`

Namespace的定义很简单，如下所示的yaml定义了名为 `development` 的 Namespace

[source, yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: development
----

一旦创建了Namespace，我们在创建资源对象时就可以指定这个资源对象属于那个Namespace。

[source, yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: development
spec:
  containers:
  - image: busybox
    command:
    - sleep
    - "3600"
  name: busybox
----

此时查看对应Namespace的Pod：`kubectl get pods --namespace=development`

=== Annotation

Annotation（注解）与Label类似，也使用key/value键值对的形式进行定义。不同的是Label具有严格的命名规则，它定义的是Kubernetes对象的元数据（Metadata），并且用户Label Selector。Annotation则是用户任意定义的附加信息，以便于外部工具查找。在很多时候，Kubernetes的模块自身会通过Annotation标记资源对象的一些特殊信息。

通常来说，用Annotation来记录的信息如下：

- build信息，release信息，Docker镜像信息，例如时间戳、release id号、PR号，镜像Hash值
- 日志库、监控库、分析库等资源库的地址信息
- 程序调试工具信息，例如工具名称、版本号等
- 团队的联系信息，例如电话号码、负责人名称、网址等

=== ConfigMap

为了集中管理系统的配置参数，而不是管理一堆配置文件。Kubernetes把所有的配置项都当作 `key-value` 字符串，当然value可以来自某个文本文件。这些配置项可以作为Map表中的一个项，整个Map的数据可以被持久化存储在Kubernetes的Etcd数据库中，然后提供API以方便Kubernetes相关组件或客户应用CRUD操作这些数据，上述专门用来保存配置参数的Map就是Kubernetes ConfigMap资源对象。

接下里Kubernetes提供了一种内建机制，将存储在etcd中的ConfigMap通过Volume映射的方式变成目标Pod内的配置文件，不管目标Pod被调度到哪台服务器上，都会完成自动映射。进一步地，如果ConfigMap中的key-value数据被修改，则映射到Pod中的配置文件也会随之更新。

== Kubernetes安装配置指南

=== kubectl命令行工具用法详解

kubectl作为客户端CLI工具，可以让用户通过命令行对Kubernetes集群进行操作。

==== kubectl用法概述

kubectl命令行的语法如下：

[source, bash]
----
kubectl [command] [TYPE] [NAME] [flags]
----

command：子命令，用于操作Kubernetes集群对象的命令，例如create、delete、describe、get、apply等

Type：资源对象的类型，区分大小写，能以单数、复数或者简写形式表示。例如以下3种TYPE是等价的

[source, bash]
----
kubectl get pod pod1
kubectl get pods pod1
kubectl get po pod1
----

NAME：资源对象的名称，区分大小写。如果不指定名称，系统则将返回属于TYPE的全部对象的列表

flags：kubectl子命令的可选参数，例如使用-s指定API Server的URL地址而不用默认值

获取多个Pod信息： `kubectl get pods pod1 pod2`

获取多种对象的信息： `kubectl get pod/pod1 rc/rc1`

同时应用多个yaml文件
[source, bash]
----
kubectl get pod -f pod1.yaml -f pod2.yaml
kubectl create -f pod1.yaml -f rc1.yaml
----

== 深入掌握Pod

=== Pod定义详解

yaml格式的Pod定义文件的完整内容如下

[source, yaml]
----
# yaml格式的pod定义文件完整内容：
apiVersion: v1        　　#必选，版本号，例如v1
kind: Pod       　　　　　　#必选，Pod
metadata:       　　　　　　#必选，元数据
  name: string        　　#必选，Pod名称
  namespace: string     　　#必选，Pod所属的命名空间
  labels:       　　　　　　#自定义标签
    - name: string      　#自定义标签名字
  annotations:        　　#自定义注释列表
    - name: string
spec:         　　　　　　　#必选，Pod中容器的详细定义
  containers:       　　　　#必选，Pod中容器列表
  - name: string      　　#必选，容器名称
    image: string     　　#必选，容器的镜像名称
    imagePullPolicy: [Always | Never | IfNotPresent]  #获取镜像的策略 Alawys表示下载镜像 IfnotPresent表示优先使用本地镜像，否则下载镜像，Nerver表示仅使用本地镜像
    command: [string]     　　#容器的启动命令列表，如不指定，使用打包时使用的启动命令
    args: [string]      　　 #容器的启动命令参数列表
    workingDir: string      #容器的工作目录
    volumeMounts:     　　　　#挂载到容器内部的存储卷配置
    - name: string      　　　#引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名
      mountPath: string     #存储卷在容器内mount的绝对路径，应少于512字符
      readOnly: boolean     #是否为只读模式
    ports:        　　　　　　#需要暴露的端口库号列表
    - name: string      　　　#端口号名称
      containerPort: int    #容器需要监听的端口号
      hostPort: int     　　 #容器所在主机需要监听的端口号，默认与Container相同
      protocol: string      #端口协议，支持TCP和UDP，默认TCP
    env:        　　　　　　#容器运行前需设置的环境变量列表
    - name: string      　　#环境变量名称
      value: string     　　#环境变量的值
    resources:        　　#资源限制和请求的设置
      limits:       　　　　#资源限制的设置
        cpu: string     　　#Cpu的限制，单位为core数，将用于docker run --cpu-shares参数
        memory: string      #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数
      requests:       　　#资源请求的设置
        cpu: string     　　#Cpu请求，容器启动的初始可用数量
        memory: string      #内存清楚，容器启动的初始可用数量
    livenessProbe:      　　#对Pod内个容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器只需设置其中一种方法即可
      exec:       　　　　　　#对Pod容器内检查方式设置为exec方式
        command: [string]   #exec方式需要制定的命令或脚本
      httpGet:        　　　　#对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port
        path: string
        port: number
        host: string
        scheme: string
        HttpHeaders:
        - name: string
          value: string
      tcpSocket:      　　　　　　#对Pod内个容器健康检查方式设置为tcpSocket方式
         port: number
       initialDelaySeconds: 0   #容器启动完成后首次探测的时间，单位为秒
       timeoutSeconds: 0    　　#对容器健康检查探测等待响应的超时时间，单位秒，默认1秒
       periodSeconds: 0     　　#对容器监控检查的定期探测时间设置，单位秒，默认10秒一次
       successThreshold: 0
       failureThreshold: 0
       securityContext:
         privileged: false
    restartPolicy: [Always | Never | OnFailure] #Pod的重启策略，Always表示一旦不管以何种方式终止运行，kubelet都将重启，OnFailure表示只有Pod以非0退出码退出才重启，Nerver表示不再重启该Pod
    nodeSelector: obeject   　　#设置NodeSelector表示将该Pod调度到包含这个label的node上，以key：value的格式指定
    imagePullSecrets:     　　　　#Pull镜像时使用的secret名称，以key：secretkey格式指定
    - name: string
    hostNetwork: false      　　#是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络
    volumes:        　　　　　　#在该pod上定义共享存储卷列表
    - name: string     　　 　　#共享存储卷名称 （volumes类型有很多种）
      emptyDir: {}      　　　　#类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值
      hostPath: string      　　#类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录
        path: string      　　#Pod所在宿主机的目录，将被用于同期中mount的目录
      secret:       　　　　　　#类型为secret的存储卷，挂载集群与定义的secre对象到容器内部
        scretname: string
        items:   
        - key: string
          path: string
      configMap:      　　　　#类型为configMap的存储卷，挂载预定义的configMap对象到容器内部
        name: string
        items:
        - key: string
          path: string
----

=== Pod的基本用法

Kubernetes要求我们自己创建的Docker镜像并以一个前台命令作为启动命令

如果两个容器为紧耦合的关系，并组合成一个整体对外提供服务时，应将这两个容器打包为一个Pod

[source, yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: redis-php
  labels:
    name: redis-php
spec:
  containers:
  - name: frontend
    image: jjjj
    ports:
    - containerPort: 80
  -name: redis
    image: llll
    ports:
    - containerPort: 6379
----

属于同一个Pod的多个容器应用之间相互访问时仅需通过localhost就可以通信，使得这一组容器被绑定在了一个环境中。

=== 静态Pod

静态Pod是又kubelet进行管理的仅存在与特定Node上的Pod。他们不能通过API Server进行管理，无法与ReplicationController、Deployment或者DaemonSet进行关联，并且kubelet无法对他们进行健康检查。静态Pod总是由Kubelet创建的，并且总在Kubelet所在的Node上运行。

静态Pod由两种创建方式：

. 配置文件方式：首先，需要设置Kubelet的启动参数 `--config`， 指定Kubelet需要监控的配置文件所在的目录，Kubelet会定期扫描该目录，并根据该目录下的 `.yaml` 或 `.json` 文件进行创建操作，删除此Pod只能到Kubelet所在机器上删除对应的配置文件即可
. HTTP方式：通过设置Kubelet的启动参数 `--manifest-url`，Kubelet将会定期从该URL地址下载Pod的定义文件，并以 `.yaml` 或 `.json` 文件的格式进行解析，然后创建Pod

=== Pod容器共享Volume

同一个Pod中的多个容器能够共享Pod级别的存储卷Volume。Volume可以被定义为各种类型，多个容器各自进行挂载操作，将一个Volume挂载为容器内部需要的目录，如图所示

image::Snipaste_2021-11-20_18-21-17.png[]

配置文件如下：

[source, yaml]
.pod-volume-applogs.yaml
----
apiVersion: v1
kind: Pod
metadata:
  name: volume-pod
spec:
  containers:
  - name: tomcat
    image: tomcat
    ports:
    - containerPort: 8080
      volumeMounts:
      - name: app-logs
        mountPath: /usr/local/tomcat/logs
  - name: busybox
    image: busybox
    command: ["sh", "-c", "tail -f /logs/catalina*.log"]
    volumeMounts:
    - name: app-logs
      mountPath: /logs
  volumes:
  - name: app-logs
    emptyDir: {}
----

这里设置的Volume名为app-logs，类型为emptyDir，挂载到tomcat容器内的 `/usr/local/tomcat/logs` 目录，同时挂载在busybox容器内的 `/logs` 目录。tomcat容器在启动后会向 `/usr/local/tomcat/logs` 目录写文件，busybox容器就可以读取其中的文件了。

=== Pod的配置管理

==== ConfigMap概述

ConfigMap供容器使用的典型用法如下：

. 生成为容器内的环境变量
. 设置容器启动命令的启动参数（需设置为环境变量）
. 以Volume的形式挂载为容器内部的文件或目录

ConfigMap以一个或多个key:value的形式保存在Kubernetes系统中供应用使用，既可以用于表示一个变量的值（例如apploglevel=info），也可以用于表示一个完整配置文件的内容（例如 `server.xml=<?xml...>...`）

可以通过yaml配置文件或者直接使用 `kubectl create configmap` 命令行的方式来创建ConfigMap

==== 创建ConfigMap资源对象

===== 通过yaml配置文件方式创建

[source, yaml]
.cm-appvars.yaml
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cm-appvars
data:
  apploglevel: info
  appdatadir: /var/data
  key-serverxml: |
    <?xml ...>...
----

执行kubectl create命令创建该ConfigMap： `kubectl create -f cm-appvars.yaml`

查看创建好的ConfigMap：
[source,bash]
----
# 获取信息
kubectl get configmap
# 获取详细信息
kubectl describe configmap cm-appvars
----

===== 通过kubectl命令行方式创建

通过 `--from-file` 参数从文件中进行创建，可以指定key的名称，也可以在一个命令行中创建包含多个key的ConfigMap，语法为： `kubectl create configmap NAME --from-file=[key=]source --from-file=[key=]source`

通过 `--from-file` 参数从目录中进行创建，该目录下的每个配置文件名都被设置为key，文件的内容被设置为value，语法为： `kubectl create configmap NAME --from-file=config-files-dir`

使用 `--from-literal` 时会从文本中进行创建，直接将指定的 `key#=value#` 创建为ConfigMap的内容，语法为： `kubectl create configmap NAME --from-literal=key1=value1 --from-literal=key2=value2`

==== 在Pod中使用ConfigMap

===== 通过环境变量方式使用ConfigMap

以前面创建的ConfigMap `cm-appvars` 为例

[source, yaml]
.cm-appvars.yaml
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cm-appvars
data:
  apploglevel: info
  appdatadir: /var/data
----

使用如下文件创建Pod之后会在容器内生成APPLOGLEVEL和APPDATADIR两个环境变量
[source, yaml]
.cm-appvars.yaml
----
apiVersion: v1
kind: Pod
metadata:
  name: cm-test-pod
spec:
  containers:
  - name: cm-test
    image: busybox
    command: ["/bin/sh", "-c", "env | grep APP"]
    env:
    - name: APPLOGLEVEL #定义环境变量的名称
      valueFrom:  # key apploglevel对应的值
        configMapKeyRef:
          name: cm-appvars
          key: apploglevel
    - name: APPDATADIR
      valueFrom:
        configMapKeyRef:
          name: cm-appvars
          key: appdatadir
----

使用如下文件创建Pod将会在容器内部生成apploglevel和appdatadir两个环境变量
[source, yaml]
.cm-appvars.yaml
----
apiVersion: v1
kind: Pod
metadata:
  name: cm-test-pod
spec:
  containers:
  - name: cm-test
    image: busybox
    command: ["/bin/sh", "-c", "env | grep APP"]
    envFrom:
    - configMapRef
      name: cm-appvars # 根据 cm-appvars中的key=value自动生成环境变量
  restartPolicy: Never
----

IMPORTANT: 需要说明的是，环境变量的名称受POSIX命名规范（[a-zA-Z_][a-zA-Z0-9_]*）约束，不能以数字开头，如果包含非法字符，则系统将跳过该环境变量的创建，并记录一个Event来提示环境变量无法生成，但并不组织Pod的启动

===== 通过volumeMount使用ConfigMap

在Pod `cm-test-app` 的定义中，将ConfigMap `cm-appconfigfiles` 中的内容以文件的形式mount到容器内部 `/configfiles` 目录下。

[source, yaml]
.cm-test-app.yaml
----
apiVersion: v1
kind: Pod
metadata:
  name: cm-test-pod
spec:
  containers:
  - name: cm-test-app
    image: kubeguide/tomcat-app:v1
    ports:
    - containerPort: 8080
    volumeMounts:
    - name: serverxml  # 引用Volume的名称
      mountPath: /configfiles # 挂载到容器内的目录
  volumes:
  - name: serverxml  # 定义Volume的名称
    configMap:
      name: cm-appconfigfiles # 使用ConfigMap cm-appconfigfiles
      item:
      - key: key-serverxml  # key=key-serverxml
        path: server.xml # value将server.xml文件名进行挂载
----

如果在引用ConfigMap时不指定items，则使用volumeMount方式在容器内的目录下为每个item都生成一个文件名为key的文件。

==== 使用ConfigMap的限制条件

- ConfigMap必须在Pod之前创建
- ConfigMap受Namespace限制，只有处于相同Namespace中的Pod才可以引用它
- ConfigMap中的配额管理还未能实现
- kubelet只支持可以被API Server管理的Pod使用ConfigMap。kubelet在本Node上通过 `--manifest-url` 或 `--config` 自动创建的静态Pod将无法引用ConfigMap。
- 在Pod对ConfigMap进行挂载操作时，在容器内部只能挂载为 **目录** ，无法挂载为 **文件** 。在挂载到容器内部后，在目录下将包含ConfigMap定义的每个item，如果在该目录下原来还有其他文件，则容器内的该目录将被挂载的ConfigMap覆盖。

=== Pod声明周期和重启策略

状态：

- Pending：API Server已经创建该Pod，但在Pod内部还有一个或多个容器的镜像没有创建，包括正在下载镜像的过程
- Running：Pod内所有容器均已创建，且至少有一个容器处于运行状态、正在启动状态或正在重启状态
- Succeeded：Pod内所有容器均成功执行后退出，且不会再重启
- Failed：Pod内所有容器均已退出，但至少有一个容器退出为失败状态
- Unknown：由于某种原因无法获取该Pod的状态，可能由于网络通信不畅导致

Pod的重启策略（RestartPolicy）,应用于Pod内的所有容器，并且仅再Pod所处的Node上由kubelet进行判断和重启操作。

- Always：当容器失效时，由kubelet自动重启该容器
- OnFailure：当容器终止运行且退出码不为0时，由kueblet自动重启该容器
- Never：不论容器运行状态如何，kubelet都不会重启该容器

kubelet重启失效容器的时间间隔以 `sync-frequency` 乘以2n来计算，例如1、2、4、8倍等，最长延时5min，并且再成功重启后的10min后重置该时间。

Pod的重启策略与控制方式息息相关。每种控制器对Pod的重启策略要求如下：

- RC和DaemonSet：必须设置为Always，需要保证该容器持续运行
- Job：OnFailuer或Never，确保容器执行完成后不再重启
- kubelet：在Pod失效时自动重启它，不论将RestartPolicy设置为什么值，也不会对Pod进行健康检查






























